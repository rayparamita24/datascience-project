# -*- coding: utf-8 -*-
"""Fake_News_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19nKsFVGGjH-b2pSsE5QHK2FyTmq0tyKk

<a href="https://colab.research.google.com/github/saanikagupta/Fake-News-Detection/blob/master/Fake_News_Detection.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

# Importing libraries
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
from keras import utils
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras import optimizers, callbacks
from keras.losses import categorical_crossentropy
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR)

"""# Preprocessing"""

# Reading the excel file into DataFrame
train = pd.read_excel('train.xlsx', header = None, names = ['id',	'label'	,'statement',	'subject',	'speaker', 	'job', 	'state',	'party',	'barely_true_c',	'false_c',	'half_true_c',	'mostly_true_c',	'pants_on_fire_c',	'venue'])
test = pd.read_excel('test.xlsx', header = None, names = ['id',	'label'	,'statement',	'subject',	'speaker', 	'job', 	'state',	'party',	'barely_true_c',	'false_c',	'half_true_c',	'mostly_true_c',	'pants_on_fire_c',	'venue'])
val = pd.read_excel('valid.xlsx', header = None, names = ['id',	'label'	,'statement',	'subject',	'speaker', 	'job', 	'state',	'party',	'barely_true_c',	'false_c',	'half_true_c',	'mostly_true_c',	'pants_on_fire_c',	'venue'])

# Dropping the 'id' column
train.drop('id', axis = 1, inplace = True)
test.drop('id', axis = 1, inplace = True)
val.drop('id', axis = 1, inplace = True)

train.head(5)

# Checking the shape of data
print(train.shape)
print(val.shape)
print(test.shape)

# list(train.columns)

# Function for cleaning the dataset
def dataPreprocessing(filename, corpus):

  length = filename.shape[0]

  # Missing values
  filename["job"].fillna("no-job", inplace = True)
  filename["state"].fillna("no-state", inplace = True)

  for x in range(length):
    statement = re.sub('[^a-zA-Z]', ' ', train['statement'][x]) # Removing all numbers and special characters
    statement = statement.lower() # Converting uppercase to lowercase
    statement = statement.split()
    ps = PorterStemmer()
    statement = [ps.stem(word) for word in statement if not word in set(stopwords.words('english'))] # Stemming the dataset and removing stopwords
    statement = ' '.join(statement)
    subject = train['subject'][x].replace(',', ' ')
    speaker = train['speaker'][x]
    job = train['job'][x].lower()
    # job = job.replace(' ', '-')
    state = train['state'][x].lower()
    party = train['party'][x].lower()
    corpus.append(statement + ' '  + subject + ' ' + job + ' ' + state + ' ' + party)
  return corpus

corpus = []
corpus = dataPreprocessing(train, corpus) # This returns a corpus containing only the train dataset
corpus = dataPreprocessing(val, corpus) # This returns a corpus containing train and val dataset
corpus = dataPreprocessing(test, corpus) # This returns a corpus containing train, val and test dataset

len(corpus) # 10269 + 1284 + 1283

# Converting the corpus into bag-of-words
cv = CountVectorizer(max_features = 8000)
X = cv.fit_transform(corpus).toarray()

X.shape

# Obtaining the x_train, x_val and x_test from the bag-of-words (As the dataset was merged during preprocessing)
x_train = X[: 10269, :]
x_val = X[10269 : 11553, :]
x_test = X[11553 : 12836, :]

# x_val.shape

# Selecting the columns 'barely_true_c',	'false_c',	'half_true_c',	'mostly_true_c',	'pants_on_fire_c'
x_train2 = train.iloc[:, 7: 12]
x_val2 = val.iloc[:, 7: 12]
x_test2 = test.iloc[:, 7: 12]

type(x_train)

print(x_train.shape)
print(x_train2.shape)

# Stacking x_train and x_train2 horizontally
x_train = np.hstack((x_train, x_train2))
x_val = np.hstack((x_val, x_val2))
x_test = np.hstack((x_test, x_test2))

x_train.shape

"""# Six-way classification

## Preprocessing
"""

num_classes = 6
# Preprocessing function for the labels
def categorize(filename):
  y = filename["label"].tolist()

  # Encoding the Dependent Variable
  labelencoder_y = LabelEncoder()
  y = labelencoder_y.fit_transform(y)

  # Converting to binary class matrix
  y = utils.to_categorical(y, num_classes)
  return y

y_train = categorize(train)
y_test = categorize(test)
y_val = categorize(val)

y_test.shape

# Checking for missing values
np.where(np.isnan(x_train))

# Checking for missing values
np.where(np.isnan(x_test))

# Checking for missing values
np.where(np.isnan(x_val))

"""## Artificial Neural Network"""

# Initializing hyperparameters
learn_rate = 0.001
batch_size = 500
epochs = 10
num_classes = 6

seed = 2
np.random.seed(seed)

# Creating the model
model = Sequential()
model.add(Dense(8005, activation = 'relu', kernel_initializer = 'glorot_uniform'))
model.add(Dropout(0.85))
model.add(Dense(121, activation = 'relu'))
model.add(Dropout(0.75))
model.add(Dense(num_classes, activation = 'softmax'))

rmsprop = optimizers.RMSprop(learn_rate)
model.compile(loss = categorical_crossentropy, optimizer = rmsprop, metrics = ['accuracy']) # Compile model

# Checkpoint
filepath = "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = callbacks.ModelCheckpoint(filepath, monitor = 'val_acc', save_best_only = False, save_weights_only = False)
callbacks_list = [checkpoint]

# Model fitting
model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, callbacks = callbacks_list, verbose = 1, validation_data = (x_val, y_val))

model.summary()

"""## Evaluation"""

# Loading weights
# epoch = 4, train_acc = 38.46%, val_acc = 41.82%
model.load_weights("weights-improvement-04-0.42.hdf5")
# test_acc = 40.84%

# Estimating the accuracy on the test dataset using loaded weights
scores = model.evaluate(x_test, y_test, verbose = 0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

# y_test
y_pred = model.predict(x_test)
# y_pred

y_pred.argmax(axis = 1)

y_test.argmax(axis = 1)

# Building the confusion matrix
matrix = metrics.confusion_matrix(y_test.argmax(axis = 1), y_pred.argmax(axis = 1))

# Confusion matrix for six-way classification
matrix

"""# Binary Classification

## Preprocessing
"""

num_classes = 2

# Function for preprocessing labels
def dataPreprocessingBinary(filename):
  y = filename["label"].tolist()

  # Changing the 'half-true', 'mostly-true', barely-true', 'pants-fire' labels to True/False for Binary Classification
  for x in range(len(y)):
    if(y[x] == 'half-true'):
       y[x] = 'True'
    elif(y[x] == 'mostly-true'):
       y[x] = 'True'
    elif(y[x] == 'barely-true'):
       y[x] = 'False'
    elif(y[x] == 'pants-fire'):
       y[x] = 'False'

  # Converting the lables into binary class matrix
  labelencoder_y = LabelEncoder()
  y = labelencoder_y.fit_transform(y)
  y = utils.to_categorical(y, num_classes)
  return y

y_train_binary = dataPreprocessingBinary(train)
y_test_binary = dataPreprocessingBinary(test)
y_val_binary = dataPreprocessingBinary(val)

"""## Artificial Neural Network"""

# Hyperparameters
learn_rate = 0.001
batch_size = 500
epochs = 20

seed = 1
np.random.seed(seed)

# Creating model
model = Sequential()
model.add(Dense(8005, activation = 'relu', kernel_initializer = 'glorot_uniform'))
model.add(Dropout(0.9))
model.add(Dense(121, activation = 'relu'))
model.add(Dropout(0.8))
model.add(Dense(num_classes, activation = 'softmax'))

rmsprop = optimizers.RMSprop(learn_rate)
model.compile(loss = categorical_crossentropy, optimizer = rmsprop, metrics = ['accuracy']) # Compiling the model

# Checkpoint
filepath = "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = callbacks.ModelCheckpoint(filepath, monitor = 'val_acc', save_best_only = True, save_weights_only = False)
callbacks_list = [checkpoint]

# Model fitting
model.fit(x_train, y_train_binary, batch_size = batch_size, epochs = epochs, callbacks = callbacks_list, verbose = 1, validation_data = (x_val, y_val_binary))

model.summary()

"""## Evaluation"""

# Loading weights
# epoch = 4, train_acc = 70.15%, val_acc = 67.6%
model.load_weights("weights-improvement-04-0.68.hdf5")
# test_acc = 69.91%

# Estimating the accuracy on the test dataset using loaded weights
scores = model.evaluate(x_test, y_test_binary, verbose = 0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

y_pred = model.predict(x_test) # Predicting y for x_test
matrix = metrics.confusion_matrix(y_test_binary.argmax(axis = 1), y_pred.argmax(axis = 1)) # Building the confusion matrix

# Confusion matrix for binary classification
matrix